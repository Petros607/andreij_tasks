# -*- coding: utf-8 -*-
"""distilbert-base-uncased(f-t).ipynb
Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/drive/1_9_nbt7C6-xDDUaPYFyeZnAkd0NcXLry
"""

# pip install transformers datasets torch scikit-learn

from datasets import Dataset
from transformers import (
    AutoTokenizer,
    DistilBertForSequenceClassification,
    TrainingArguments,
    Trainer,
)
import torch
import numpy as np
import pandas as pd
from sklearn.metrics import classification_report, accuracy_score

MODEL_NAME = "distilbert-base-uncased"
OUTPUT_DIR = "./fine_tuned_model"
# TRAIN_SAMPLES = 800
# EVAL_SAMPLES = 50
# TEST_SAMPLES = 50
TRAIN_SAMPLES = 80
EVAL_SAMPLES = 5
TEST_SAMPLES = 5
SEED = 42

def load_restaurant_dataset():
    """
    Load and split the restaurant reviews dataset into train, eval, and test sets.
    """
    df = pd.read_csv('Restaurant_Reviews.csv', names=["text", "label"], header=0)
    df["label"] = df["label"].astype(int)
    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)

    train_df = df[:TRAIN_SAMPLES]
    eval_df = df[TRAIN_SAMPLES:TRAIN_SAMPLES + EVAL_SAMPLES]
    test_df = df[TRAIN_SAMPLES + EVAL_SAMPLES:TRAIN_SAMPLES + EVAL_SAMPLES + TEST_SAMPLES]

    return (
        Dataset.from_pandas(train_df),
        Dataset.from_pandas(eval_df),
        Dataset.from_pandas(test_df),
    )

def tokenize_dataset(dataset, tokenizer):
    """
    Tokenize a Kaggle using the provided tokenizer.
    """
    return dataset.map(
        lambda examples: tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=128
        ),
        batched=True,
        num_proc=4,
        remove_columns=["text"]
    ).with_format("torch")

def fine_tune_model(train_dataset, eval_dataset, output_dir):
    """
    Fine-tune DistilBERT on the training data.
    """
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    tokenized_train = tokenize_dataset(train_dataset, tokenizer)
    tokenized_eval = tokenize_dataset(eval_dataset, tokenizer)

    model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    training_args = TrainingArguments(
        output_dir=output_dir,
        eval_strategy="epoch",
        save_strategy="epoch",
        num_train_epochs=5,
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        learning_rate=2e-5,
        weight_decay=0.01,
        logging_dir='./logs',
        #logging_steps=10,
        logging_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_eval,
        tokenizer=tokenizer
    )

    trainer.train()
    eval_results = trainer.evaluate()
    print(f"Evaluation results: {eval_results}")

    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"Model saved to {output_dir}")

def load_model(model_dir):
    """
    Load a fine-tuned model and tokenizer from directory.
    """
    model = DistilBertForSequenceClassification.from_pretrained(model_dir)
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    print("Model and tokenizer loaded successfully!")
    return model, tokenizer

def evaluate_model(model, tokenizer, test_dataset):
    """
    Evaluate the model on a test dataset and print accuracy and classification report.
    """
    tokenized_test = tokenize_dataset(test_dataset, tokenizer)
    trainer = Trainer(model=model)
    predictions = trainer.predict(tokenized_test)
    predicted_labels = torch.argmax(torch.tensor(predictions.predictions), dim=-1).numpy()
    true_labels = np.array([example["label"] for example in test_dataset])

    accuracy = accuracy_score(true_labels, predicted_labels)
    report = classification_report(true_labels, predicted_labels, target_names=["Negative", "Positive"])

    print(f"Classification Accuracy: {accuracy:.4f}")
    print("Classification Report:\n", report)

if __name__ == "__main__":
    train_dataset, eval_dataset, test_dataset = load_restaurant_dataset()
    fine_tune_model(train_dataset, eval_dataset, OUTPUT_DIR)
    model, tokenizer = load_model(OUTPUT_DIR)
    evaluate_model(model, tokenizer, test_dataset)
